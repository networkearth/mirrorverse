{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AWS_PROFILE'] = 'admin'\n",
    "os.environ['HAVEN_DATABASE'] = 'haven'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from random import sample\n",
    "import h3\n",
    "\n",
    "from mirrorverse.utils import read_data_w_cache\n",
    "from haven.db import write_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we sample down to a 15 minute interval\n",
    "\n",
    "data = read_data_w_cache(\n",
    "    '''\n",
    "    select distinct\n",
    "        tag_key,\n",
    "        first_value(depth) over (partition by tag_key, epoch - epoch % 900 order by epoch asc) as depth,\n",
    "        epoch - epoch % 900 as epoch\n",
    "    from \n",
    "        mgietzmann_tag_depths \n",
    "    '''\n",
    ")\n",
    "data = data[~np.isnan(data['depth'])]\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth_class(depth_classes, depth):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - depth_classes: np.array, the depth classes to choose from\n",
    "    - depth: float, the depth of the fish as recorded\n",
    "\n",
    "    Outputs:\n",
    "    - int, the selected depth class\n",
    "\n",
    "    Selects a depth class based on the depth of the fish.\n",
    "\n",
    "    It turns out that PSAT summary data bins the depth into\n",
    "    intervals so the actual depth is not known. However\n",
    "    given the recorded depth we can estimate the depth classes\n",
    "    it could belong to and the likelihoods of each.\n",
    "    \"\"\"\n",
    "    depth_classes = np.array(depth_classes)\n",
    "\n",
    "    sd = (\n",
    "        depth * 0.08 / 1.96\n",
    "    )  # ~two standard deviations gives our 95% confidence interval\n",
    "    if sd == 0:\n",
    "        division = np.zeros(len(depth_classes))\n",
    "        division[0] = 1\n",
    "    else:\n",
    "        # we're going to assume the depth classes are sorted\n",
    "        z = (depth_classes - depth) / sd\n",
    "        division = norm.cdf(z)\n",
    "        division[1:] = division[1:] - division[:-1]\n",
    "    #print(depth, division)\n",
    "    # if there aren't quite enough depth classes the\n",
    "    # probabilities may not sum to 1, so we'll normalize\n",
    "    division = division / division.sum()\n",
    "    return float(np.random.choice(depth_classes, p=division))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_bins = [25, 50, 75, 100, 150, 200, 250, 300, 400, 500]\n",
    "\n",
    "data['depth_bin'] = data['depth'].apply(lambda depth: get_depth_class(depth_bins, depth))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_keys = list(data['tag_key'].unique())\n",
    "test_keys = sample(tag_keys, int(len(tag_keys) * 0.35))\n",
    "print(len(test_keys))\n",
    "train_keys = [key for key in tag_keys if key not in test_keys]\n",
    "print(len(train_keys))\n",
    "\n",
    "keys_df = pd.concat([\n",
    "    pd.DataFrame({'tag_key': train_keys, '_train': [True] * len(train_keys)}),\n",
    "    pd.DataFrame({'tag_key': test_keys, '_train': [False] * len(test_keys)})\n",
    "]).reset_index(drop=True).reset_index().rename({'index': '_individual'}, axis=1)\n",
    "keys_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(keys_df)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values('_individual').reset_index(drop=True).reset_index().rename({'index': '_decision'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = read_data_w_cache(\n",
    "    'select tag_key, epoch, longitude, latitude from mgietzmann_tag_tracks'\n",
    ")\n",
    "print(tracks.shape)\n",
    "tracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks['h3_index'] = tracks.apply(\n",
    "    lambda r: h3.geo_to_h3(r['latitude'], r['longitude'], resolution=4), \n",
    "    axis=1\n",
    ")\n",
    "tracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks['time'] = pd.to_datetime(tracks['epoch'], unit='s').dt.date\n",
    "data['time'] = pd.to_datetime(data['epoch'], unit='s').dt.date\n",
    "\n",
    "data = data.merge(tracks[['tag_key', 'time', 'h3_index']], on=['tag_key', 'time'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data(\n",
    "    data, 'chinook_depth_decisions', ['_train']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from haven.db import drop_table\n",
    "#\n",
    "#drop_table('chinook_depth_decisions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

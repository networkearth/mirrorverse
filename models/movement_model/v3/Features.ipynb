{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import h3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import haven.db as db \n",
    "import geopy.distance\n",
    "import plotly.express as px\n",
    "\n",
    "os.environ['HAVEN_DATABASE'] = 'haven'\n",
    "os.environ['AWS_PROFILE'] = 'admin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "with neighbors as (\n",
    "    select \n",
    "        *,\n",
    "        row_number() over () as _choice\n",
    "    from \n",
    "        movement_model_neighbors\n",
    "    where\n",
    "        max_km = 100\n",
    ")\n",
    "select \n",
    "    n.current_h3_index,\n",
    "    n.selected_h3_index,\n",
    "    n.neighbor_h3_index,\n",
    "    n.date,\n",
    "    n._individual,\n",
    "    n._decision,\n",
    "    n._choice,\n",
    "    n.tag_key,\n",
    "    p.mixed_layer_thickness,\n",
    "    p.velocity_east,\n",
    "    p.velocity_north,\n",
    "    c.net_primary_production\n",
    "from \n",
    "    neighbors n \n",
    "    inner join copernicus_physics p \n",
    "        on p.depth_bin = 25 \n",
    "        and p.region = 'chinook_study'\n",
    "        and n.neighbor_h3_index = p.h3_index \n",
    "        and n.date = p.date \n",
    "    inner join copernicus_biochemistry c\n",
    "        on c.depth_bin = 25 \n",
    "        and c.region = 'chinook_study'\n",
    "        and n.neighbor_h3_index = c.h3_index \n",
    "        and n.date = c.date \n",
    "'''\n",
    "\n",
    "cached_file_path = 'cached_model.snappy.parquet'\n",
    "if os.path.exists(cached_file_path):\n",
    "    data = pd.read_parquet(cached_file_path)\n",
    "else:\n",
    "    data = db.read_data(sql)\n",
    "data.to_parquet(cached_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Spatial Features of Each Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['current_lat'] = data['current_h3_index'].apply(lambda i: h3.h3_to_geo(i)[0])\n",
    "data['current_lon'] = data['current_h3_index'].apply(lambda i: h3.h3_to_geo(i)[1])\n",
    "data['neighbor_lat'] = data['neighbor_h3_index'].apply(lambda i: h3.h3_to_geo(i)[0])\n",
    "data['neighbor_lon'] = data['neighbor_h3_index'].apply(lambda i: h3.h3_to_geo(i)[1])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(row):\n",
    "    return geopy.distance.geodesic(\n",
    "        (row['current_lat'], row['current_lon']),\n",
    "        (row['neighbor_lat'], row['neighbor_lon'])\n",
    "    ).km\n",
    "\n",
    "data['distance'] = data.apply(get_distance, axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['water_heading'] = data.apply(lambda r: np.arctan2(r['velocity_north'], r['velocity_east']), axis=1)\n",
    "data['movement_heading'] = data.apply(\n",
    "    lambda r: np.arctan2(\n",
    "        r['neighbor_lat'] - r['current_lat'],\n",
    "        r['neighbor_lon'] - r['current_lon'] \n",
    "    ) if r['distance'] else 0, axis=1\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to Decisions that are Legal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "data = data.dropna()\n",
    "print(data.shape)\n",
    "data['_selected'] = data['selected_h3_index'] == data['neighbor_h3_index']\n",
    "gdf = data.groupby('_decision')[['_selected']].max().reset_index()\n",
    "print(gdf.shape)\n",
    "gdf = gdf[gdf['_selected']]\n",
    "print(gdf.shape)\n",
    "data = data.merge(gdf[['_decision']], how='inner')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['normed_distance'] = data['distance'] / 100\n",
    "px.histogram(\n",
    "    data['normed_distance']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['log_npp'] = np.log(data['net_primary_production'] + 0.001)\n",
    "data['log_npp_mean'] = data['log_npp'].mean()\n",
    "print(data['log_npp'].mean())\n",
    "data['normed_log_npp'] = (data['log_npp'] - data['log_npp'].mean())\n",
    "px.histogram(\n",
    "    data['normed_log_npp']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['log_mlt'] = np.log(data['mixed_layer_thickness'] + 0.001)\n",
    "data['log_mlt_mean'] = data['log_mlt'].mean()\n",
    "print(data['log_mlt'].mean())\n",
    "data['normed_log_mlt'] = (data['log_mlt'] - data['log_mlt'].mean())\n",
    "px.histogram(\n",
    "    data['normed_log_mlt']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Training/Testing Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['_train'] = data['_individual'] % 3 > 0\n",
    "data[data['_train']]['_individual'].drop_duplicates().shape[0] / data[~data['_train']]['_individual'].drop_duplicates().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_upload = data[[\n",
    "    '_individual', '_decision', '_choice', '_selected', '_train', # required by mimic\n",
    "    'normed_log_mlt', 'normed_log_npp', 'normed_distance', 'water_heading', 'movement_heading', # features\n",
    "    'log_mlt_mean', 'log_npp_mean', # for build new features\n",
    "    'mixed_layer_thickness', 'net_primary_production', 'distance', # unnormalized\n",
    "    'neighbor_h3_index', 'tag_key', 'date', # for plotting later\n",
    "]].rename({'neighbor_h3_index': 'h3_index'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.write_data(\n",
    "    to_upload, 'movement_model_features_v3', ['_train']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_upload.groupby(['_individual', '_decision'])[['_choice']].nunique().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_upload.groupby('_individual')[['_decision']].nunique().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

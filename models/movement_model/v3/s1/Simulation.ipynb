{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import haven.db as db\n",
    "import boto3\n",
    "import tensorflow.keras as keras\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['AWS_PROFILE'] = 'admin'\n",
    "os.environ['HAVEN_DATABASE'] = 'haven'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(space, experiment_name, run_id):\n",
    "    bucket_name = f\"{space}-models\"\n",
    "    model_key = f\"{experiment_name}/{run_id}/model.keras\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    s3.download_file(bucket_name, model_key, \"model.keras\")\n",
    "\n",
    "    return keras.models.load_model(\"model.keras\")\n",
    "\n",
    "model = load_model(\n",
    "    'mimic-log-odds', 'movement-model-experiment-v3-s1', \n",
    "    'e864f08e675a8bd39b0764be4827adf827b49064ed473695c4509cf0cabda693'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = pd.DataFrame([\n",
    "    {'_quanta': 10.0, 'h3_index': '840c9ebffffffff', 'date': datetime(2020, 4, 17)},\n",
    "    {'_quanta': 10.0, 'h3_index': '840c699ffffffff', 'date': datetime(2020, 4, 17)}\n",
    "])\n",
    "CONTEXT = {\n",
    "    'max_km': 100.0, \n",
    "    'mean_log_npp': 1.9670798, \n",
    "    'mean_log_mlt': 3.0952279761654187,\n",
    "    'features': [\n",
    "        \"normed_log_mlt\", \"normed_log_npp\", \"normed_distance\", \n",
    "        \"water_heading\", \"movement_heading\"\n",
    "    ],\n",
    "    'essential': [\n",
    "        'date', 'h3_index'\n",
    "    ],\n",
    "    'min_quanta': 0.01\n",
    "}\n",
    "INPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "- Initial Integrity Checks (all dates are the same)\n",
    "- Choice Expansion\n",
    "- Check of Deduplication (After Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h3\n",
    "import geopy.distance\n",
    "\n",
    "\n",
    "def find_neighbors(h3_index, max_km):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - h3_index (str): the H3 index\n",
    "\n",
    "    Finds all the h3 indices whose centroids are \n",
    "    within `max_km`. \n",
    "    \"\"\"\n",
    "    h3_coords = h3.h3_to_geo(h3_index)\n",
    "    checked = set([h3_index])\n",
    "    neighbors = set([h3_index])\n",
    "    distance = 1\n",
    "    found_neighbors = True\n",
    "\n",
    "    while found_neighbors:\n",
    "        found_neighbors = False\n",
    "        candidates = h3.k_ring(h3_index, distance)\n",
    "        new_candidates = set(candidates) - checked\n",
    "        for candidate in new_candidates:\n",
    "            if geopy.distance.geodesic(h3_coords, h3.h3_to_geo(candidate)).km <= max_km:\n",
    "                neighbors.add(candidate)\n",
    "                found_neighbors = True\n",
    "            checked.add(candidate)\n",
    "        distance += 1\n",
    "    return list(neighbors)\n",
    "\n",
    "def h3_index_expand(input, context):\n",
    "    max_km = context['max_km']\n",
    "    neighbors_rows = []\n",
    "    input = input.reset_index(drop=True).reset_index().rename({'index': '_decision', 'h3_index': 'origin_h3_index'}, axis=1)\n",
    "    for h3_index in input['origin_h3_index']:\n",
    "        neighbors = find_neighbors(h3_index, max_km)\n",
    "        neighbors_rows.extend([\n",
    "            {'origin_h3_index': h3_index, 'h3_index': neighbor}\n",
    "            for neighbor in neighbors\n",
    "        ])\n",
    "    neighbors_df = pd.DataFrame(neighbors_rows)\n",
    "    neighbors_df = neighbors_df.reset_index(drop=True).reset_index().rename({'index': '_choice'}, axis=1)\n",
    "    return input.merge(neighbors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_physics(input, context):\n",
    "    h3_indices = ','.join([f\"'{h3_index}'\" for h3_index in input['h3_index'].unique()])\n",
    "    date = input['date'].dt.strftime('%Y-%m-%d').values[0]\n",
    "    sql = f'''\n",
    "    select \n",
    "        h3_index, \n",
    "        mixed_layer_thickness,\n",
    "        velocity_east,\n",
    "        velocity_north\n",
    "    from \n",
    "        copernicus_physics\n",
    "    where \n",
    "        date = '{date}'\n",
    "        and h3_index in ({h3_indices})\n",
    "        and depth_bin = 25.0\n",
    "    '''\n",
    "    physics = db.read_data(sql)\n",
    "    return input.merge(physics, how='inner', on='h3_index')\n",
    "\n",
    "def pull_biochemistry(input, context):\n",
    "    h3_indices = ','.join([f\"'{h3_index}'\" for h3_index in input['h3_index'].unique()])\n",
    "    date = input['date'].dt.strftime('%Y-%m-%d').values[0]\n",
    "    sql = f'''\n",
    "    select \n",
    "        h3_index, \n",
    "        net_primary_production\n",
    "    from \n",
    "        copernicus_biochemistry\n",
    "    where \n",
    "        date = '{date}'\n",
    "        and h3_index in ({h3_indices})\n",
    "        and depth_bin = 25.0\n",
    "    '''\n",
    "    biochemistry = db.read_data(sql)\n",
    "    return input.merge(biochemistry, how='inner', on='h3_index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lon_lats(input, context):\n",
    "    data = input.copy()\n",
    "    data['origin_lat'] = data['origin_h3_index'].apply(lambda i: h3.h3_to_geo(i)[0])\n",
    "    data['origin_lon'] = data['origin_h3_index'].apply(lambda i: h3.h3_to_geo(i)[1])\n",
    "    data['lat'] = data['h3_index'].apply(lambda i: h3.h3_to_geo(i)[0])\n",
    "    data['lon'] = data['h3_index'].apply(lambda i: h3.h3_to_geo(i)[1])\n",
    "    return data\n",
    "\n",
    "def add_distance(input, context): \n",
    "    data = input.copy()\n",
    "    data['distance']  = data.apply(lambda r: geopy.distance.geodesic(\n",
    "        (r['origin_lat'], r['origin_lon']),\n",
    "        (r['lat'], r['lon'])\n",
    "    ).km, axis=1)\n",
    "    return data\n",
    "\n",
    "def add_headings(input, context):\n",
    "    data = input.copy()\n",
    "    data['water_heading'] = data.apply(lambda r: np.arctan2(r['velocity_north'], r['velocity_east']), axis=1)\n",
    "    data['movement_heading'] = data.apply(\n",
    "        lambda r: np.arctan2(\n",
    "            r['lat'] - r['origin_lat'],\n",
    "            r['lon'] - r['origin_lon'] \n",
    "        ) if r['distance'] else 0, axis=1\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input, CONTEXT):\n",
    "    data = input.copy()\n",
    "    data['normed_distance'] = data['distance'] / CONTEXT['max_km']\n",
    "    data['normed_log_npp'] = np.log(data['net_primary_production']) - CONTEXT['mean_log_npp']\n",
    "    data['normed_log_mlt'] = np.log(data['mixed_layer_thickness']) - CONTEXT['mean_log_mlt']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(input, model, CONTEXT):\n",
    "    data = input.copy()\n",
    "    data['odds'] = np.exp(model(data[CONTEXT['features']]))\n",
    "    data['sum_odds'] = data.groupby('_decision')['odds'].transform('sum')\n",
    "    data['probability'] = data['odds'] / data['sum_odds']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group(input, CONTEXT):\n",
    "    data = input.copy()\n",
    "    data['_quanta'] = data['_quanta'] * data['probability']\n",
    "    data['total_quanta'] = data.groupby('_decision')['_quanta'].transform('sum')\n",
    "    data = data[data['_quanta'] >= CONTEXT['min_quanta']]\n",
    "    data['remaining_quanta'] = data.groupby('_decision')['_quanta'].transform('sum')\n",
    "    data['_quanta'] = data['_quanta'] * (data['total_quanta'] / data['remaining_quanta'])\n",
    "    return data.groupby(CONTEXT['essential'])[['_quanta']].sum().reset_index()\n",
    "\n",
    "def step_forward(input, CONTEXT):\n",
    "    data = input.copy()\n",
    "    data['date'] = data['date'] + pd.DateOffset(days=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(input, model, context):\n",
    "    # expand choices\n",
    "    choices = h3_index_expand(input, context)\n",
    "    # add environmental features\n",
    "    environment = pull_physics(choices, context)\n",
    "    environment = pull_biochemistry(environment, context)\n",
    "    # add derived features\n",
    "    derived = add_lon_lats(environment, context)\n",
    "    derived = add_distance(derived, context)\n",
    "    derived = add_headings(derived, context)\n",
    "    # normalize features\n",
    "    normed = normalize(derived, context)\n",
    "    # apply model\n",
    "    results = run_model(normed, model, context)\n",
    "    # decompose\n",
    "    grouped = group(results, context)\n",
    "    return step_forward(grouped, context)\n",
    "\n",
    "results = [INPUT]\n",
    "for _ in tqdm(range(50)):\n",
    "    input = results[-1]\n",
    "    results.append(step(input, model, CONTEXT))\n",
    "\n",
    "results = pd.concat(results)\n",
    "print(results.shape)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mirrorverse.plotting import plot_h3_slider, plot_h3_animation\n",
    "\n",
    "plot_h3_animation(\n",
    "    results,\n",
    "    value_col='_quanta',\n",
    "    h3_col='h3_index',\n",
    "    slider_col='date',\n",
    "    zmax=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['date'] == datetime(2020, 6, 6)]['_quanta'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

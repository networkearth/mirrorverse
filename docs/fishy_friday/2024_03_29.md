# March 29, 2024

## Changeset

### Predicting Utility

As I've stated elsewhere we're not actually interested in predicting the most likely choice but instead want to predict the likelihoods of all the choices in front of a creature. Why? Because we want to incorporate this variability and error into our model (as our model is supposed to be a generative process). However we're not predicting the probability for a series of choices directly. Why not? For two reasons. Let's start with the first. 

Suppose we predicted, for each individual decision the probability that that decision is selected. Because we're doing this independently of the other decisions this will end up being the probability that the decision is selected globally. This immediately removes any possibility that we can flex the probability on the basis of what other choices are present. There is one probability for this decision, period. So, say I'm presented with a bunch of bad decisions. All the probabilities are low and perhaps sum to less than one. Then actually, all my probabilities are wrong for this decision and my prediction is therefore off. I'm using my predictions in a way that violates how they were trained.

Okay so suppose instead we didn't try to predict in isolation - instead we went ahead and predicted for all choices at once. Here, in our case enters the curse of dimensionality. Suppose you're trying to pick a head for a migratory movement. There are quite a few choices in front of you and all of them are going to have a series of features attendant to them. This quickly results in loads of features, and the more features you have the more flexible your model needs to be, and the more flexible your model the more data you need - and in the case of individual creature biology there often pretty hard limits on the amount of information available. Therefore it'd really be much better for the accuracy of our model if we could predict choice by choice instead of across all choices at once. 

Enter a utility function. A utility function gives you a value for each choice that is indeed global. But, instead of taking it at face value as a probability, you instead use it as a kind of odds. If the utility for one choice is 50 and for another 25 you will choose the former twice as often as the latter. So to derive the probabilities of selecting any one of our choices in a given decision we simply divide our individual utility values by the sum of utility across our choices (for that specific decision). This allows our probability to vary with our context while still having one single global value.

There's just one problem. We have no data that would allow us to train a utility model because utility is about relative odds and all we have are 1's and 0's (for choices selected and choices passed). 

Now you might imagine that we could group our like choices together and start computing ratios in some way but in the case of continuous choices this doesn't work out so well - indeed we may never see a specific choice made twice (maybe everyone always chooses a slightly different direction to move in). Point being we cannot depend on getting ratios we can train a model with directly from our data. So what are we to do?

I believe there's an odd game we can play to iterate ourselves into a dataset. To see how this would work let's take an example.

Let's suppose I've gathered data on 5 decisions I've made about what to eat in the past week. In the first I had a choice between pizza and a sandwich and I decided to order a sandwich (we'll represent this as 0, 1 for no to pizza and yes to sandwich). Then in the second choice I had no choice and could only get pizza (represented as 1, \_). In the third and fourth I had pizza and no sandwich (1, 0). And finally I went to a sub shop and got a sandwich because it was the only option (\_, 1).

Now let's suppose I give a utility of 1 to each of my options. We'd end up with something like the following:

| Decision | 0, 1 | 1, _ | 1, 0 | 1, 0 | _, 1 |
| --- | --- | --- | --- | --- | --- |
| Round 1 |
| Utility | 1, 1 | 1, _ | 1, 1| 1, 1| _, 1|
| Probabilities| .5, .5 | 1, _ | .5, .5 | .5, .5 | _, 1 |

Now here comes the game. First, suppose that for whichever was selected you "win" the difference between 1 and what you predicted the probability was and for each that wasn't selected you "lose" whatever probability you predicted.

| Decision | 0, 1 | 1, _ | 1, 0 | 1, 0 | _, 1 |
| --- | --- | --- | --- | --- | --- |
| Round 1 |
| Utility | 1, 1 | 1, _ | 1, 1| 1, 1| _, 1|
| Probabilities| .5, .5 | 1, _ | .5, .5 | .5, .5 | _, 1 |
| Wins/Losses | -0.5, 0.5 | 0, _ | 0.5, -0.5 | 0.5, -0.5 | _, 0|

Now for each column we take the utility for each choice and modify it as:

$$U_{i+1} = U_{i}(1 + w)$$

where $w$ is what it won or lost.

| Decision | 0, 1 | 1, _ | 1, 0 | 1, 0 | _, 1 |
| --- | --- | --- | --- | --- | --- |
| Round 1 |
| Utility | 1, 1 | 1, _ | 1, 1| 1, 1| _, 1|
| Probabilities| .5, .5 | 1, _ | .5, .5 | .5, .5 | _, 1 |
| Wins/Losses | -0.5, 0.5 | 0, _ | 0.5, -0.5 | 0.5, -0.5 | _, 0|
| New Utility | 0.5, 1.5 | 1, _ | 1.5, 0.5 | 1.5, 0.5 | _, 1|

So now what? Well the last part of this game would be to train our model on the "New Utility" column. Assuming, for now, that our model predicts one utility for pizza and one for sandwich we have:

$$pizza=(0.5 + 1 + 1.5 + 1.5)/4=1.125$$
$$sandwich=(1.5 + 0.5 + 0.5 + 1)/4=0.875$$

So we increased the utility of pizza and dropped the utility of sandwiches which makes sense given that pizza is indeed the more likely option when paired with sandwich as a choice. So far so good! But what happens when we get to the utility we would expect (2 vs 1)?

| Decision | 0, 1 | 1, _ | 1, 0 | 1, 0 | _, 1 |
| --- | --- | --- | --- | --- | --- |
| Round N |
| Utility | 2, 1 | 2, _ | 2, 1| 2, 1| _, 1|
| Probabilities| 2/3, 1/3 | 1, _ | 2/3, 1/3 | 2/3, 1/3 | _, 1 |
| Wins/Losses | -2/3, 2/3 | 0, _ | 1/3, -1/3 | 1/3, -1/3 | _, 0|
| New Utility | 2/3, 5/3 | 2, _ | 8/3, 2/3 | 8/3, 2/3 | _, 1|

$$pizza=(2/3 + 2 + 8/3 + 8/3)/4=2$$
$$sandwich=(5/3 + 2/3 + 2/3 + 1)/4=1$$

The overall utility (from our model) doesn't change!

And what about if we shoot past and choose (3, 1) as our utilities?

| Decision | 0, 1 | 1, _ | 1, 0 | 1, 0 | _, 1 |
| --- | --- | --- | --- | --- | --- |
| Round M |
| Utility | 3, 1 | 3, _ | 3, 1| 3, 1| _, 1|
| Probabilities| 3/4, 1/4 | 1, _ | 3/4, 1/4 | 3/4, 1/4 | _, 1 |
| Wins/Losses | -3/4, 3/4 | 0, _ | 1/4, -1/4 | 1/4, -1/4 | _, 0|
| New Utility | 3/4, 7/4 | 3, _ | 15/4, 3/4 | 15/4, 3/4 | _, 1|

$$pizza=(3/4 + 3 + 15/4 + 15/4)/4=2.8125$$
$$sandwich=(7/4 + 3/4 + 3/4 + 1)/4=1.0625$$

Which is once again in the right direction. 

The basic idea here is that your utility is changed heavily when it is placed in a decision that is counter to itself. So if the utility is very high but is placed in a decision where it's corresponding choice was not made the game attempts to drop the utility quite dramatically. Likewise if the utility was very low but placed in a decision where it's choice was selected then the game attempts to bump up the utility dramatically. However if the utility is generally on the money, for each large failure, there will be several small successes to balance it out. However as you get closer and closer to being right on (i.e. predicting 100% probability) in those wins not only do your failures get larger but your individual wins get smaller. And therefore upon repeating this game over and over your utility will "search" for a balance. 

This update function:

$$U_{i+1} = U_{i}(1 + w)$$

means that the balance will be found when the effective sum of your wins and losses are zero (assuming your model is solving for means) which happens to be the balance point we want. 

So let's formalize the steps of the game.

1. Assume even utility across the board.
2. Create the "New Utility" row.
3. Train a model that aims for means on the "New Utility" row.
4. Play another round with the new utilities. 

If we keep playing this game eventually we'll converge to the real utility function. What's awesome about this is that so long as our model is aiming for means (and therefore additively weighting wins and losses) we would expect this process to converge regardless of the model we are using! If our model predicts the utility too high on some set of choices it will get overall losses and therefore will get a new dataset that will force a drop in the utility function for those choices. If it predicts too low the opposite will happen. Therefore this process should allow the dataset+model pairing to correct itself. Indeed we expect this process to be nice and smooth because updates near the optimum will be very small (as at the optimum the update is zero). 

So really all that's left for us to understand is how to look for convergence. 

For this we have something nice and convenient. We can actually just compare each model to the subsequent "New Utility". Remember the errors here should be getting smaller and smaller and then once the model can't resolve differences in "New Utility" anymore the error will just flatline. This then can be used as a kind of learning curve to see whether or not we're making any progress.






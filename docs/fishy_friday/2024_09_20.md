# September 20, 2024

## Demo!

### Haven

```yaml
database: demo
account: '575101084097'
region: 'us-east-1'
```

```bash
haven init config.yaml
```

```python
import os
import haven.db as db
os.environ['HAVEN_DATABASE'] = 'demo'
os.environ['AWS_PROFILE'] = 'admin'

import pandas as pd
data = pd.DataFrame([
    {'name': 'Alice', 'age': 25, 'city': 'New York'},
    {'name': 'Bob', 'age': 30, 'city': 'New York'},
    {'name': 'Charlie', 'age': 35, 'city': 'Boston'},
    {'name': 'David', 'age': 40, 'city': 'Boston'},
])

db.write_data(data, 'people', ['city'])

db.drop_table('people')
```

### WaterCycle

```
watercycle deploy job
```

### Mimic

#### Building a Dataset

```json
{
    "database": "haven",
    "table": "depth_model_features_mk3",
    "train_partitions": 40,
    "test_partitions": 20,
    "max_choices": 10,
    "features": ["depth_class", "daytime", "month", "period_progress", "elevation"],
    "missing_values_map": {
        "depth_class": -1000,
        "daytime": -1000,
        "month": -1000,
        "period_progress": -1000,
        "elevation": -1000
    },
    "space": "mimic-log-odds",
    "dataset": "depth-model-mk3"
}
```

```bash
mimic log-odds build-dataset dataset.json
```

#### Training a Model

```json
{
    "experiment_name": "depth-model-mk3-session1",
    "space": "mimic-log-odds",
    "dataset": "depth-model-mk3",
    "max_choices": 10,
    "features": ["depth_class", "daytime", "month", "period_progress", "elevation"],
    "database": "haven",
    "table": "depth_model_mk3_session1",
    "models": [
        {
            "batch_size": 5000,
            "epochs": 15,
            "layers": [
                "D64",
                "D32",
                "D16"
            ]
        },
        {
            "batch_size": 10000,
            "epochs": 15,
            "layers": [
                "D16",
                "D8",
                "D4"
            ]
        }
    ]
}
```

```python
from tensorflow.keras.layers import Dense

LAYERS = {
    "D4": lambda: Dense(4, activation='relu'),
    "D8": lambda: Dense(8, activation='relu'),
    "D16": lambda: Dense(16, activation='relu'),
    "D32": lambda: Dense(32, activation='relu'),
    "D64": lambda: Dense(64, activation='relu'),
}
```

```bash
mimic log-odds run-experiment experiment.json layers.py
```

#### Running the Model

```json
{
    "database": "haven",
    "table": "depth_model_hypercube_mk3",
    "train_partitions": 0,
    "test_partitions": 4,
    "max_choices": 10,
    "features": ["depth_class", "daytime", "month", "period_progress", "elevation"],
    "missing_values_map": {
        "depth_class": -1000,
        "daytime": -1000,
        "month": -1000,
        "period_progress": -1000,
        "elevation": -1000
    },
    "space": "mimic-log-odds",
    "experiment_name": "depth-model-mk3-session1",
    "run_id": "697dc29ac80163163d968ef8dfb892878c84a650772949282d6e9b70cc06b5df",
    "upload_table": "depth-model-mk3-hypercube-inference"
}
```

```bash
mimic log-odds run-batch-infer infer.json
```

### Backup

```sql
CREATE TABLE "haven"."depth_model_features_mk6" WITH (
  format = 'parquet',
  external_location = 's3://haven-database/depth_model_features_mk6/',
  write_compression = 'SNAPPY'
) AS with elevation as (
  select h3_index,
    elevation
  from mean_elevation_by_h3
  where h3_resolution = 4
),
tag_features as (
  select p.tag_key,
    p.h3_index,
    p.sunrise,
    p.sunset,
    d.epoch,
    p.lon,
    p.lat,
    extract(
      HOUR
      from from_unixtime(d.epoch)
    ) as hour,
    extract(
      MONTH
      from from_unixtime(d.epoch)
    ) as month,
    d.selected_depth_class
  from tag_position_features_mk1 as p
    inner join tag_depth_features_mk1 as d on p.tag_key = d.tag_key
    and p.epoch = (d.epoch - d.epoch % (24 * 3600))
    and p.tag_key = d.tag_key
),
joined as (
  select t.tag_key,
    t.selected_depth_class,
    t.epoch,
    t.lon,
    t.lat,
    (
      (
        hour < sunset
        and hour >= sunrise
      )
      or (
        hour >= sunrise
        and sunrise > sunset
      )
      or (
        hour < sunset
        and sunrise > sunset
      )
    ) as daytime,
    case
      when sunrise < sunset then sunset - sunrise else 24 - sunrise + sunset
    end as daytime_length,
    t.hour,
    t.month,
    t.sunrise as sunrise,
    t.sunset as sunset,
    e.elevation
  from tag_features as t
    inner join elevation as e on t.h3_index = e.h3_index
),
features as (
  select tag_key,
    epoch,
    lon,
    lat,
    selected_depth_class,
    row_number() over () as _decision,
    case
      when daytime
      and hour >= sunrise then cast((hour - sunrise) as double) / daytime_length
      when daytime
      and hour < sunrise then cast((hour + 24 - sunrise) as double) / daytime_length
      when not daytime
      and hour >= sunset then cast((hour - sunset) as double) / (24 - daytime_length) else cast((hour + 24 - sunset) as double) / (24 - daytime_length)
    end as period_progress,
    daytime,
    elevation,
    month
  from joined
),
expanded_features as (
  select *
  from unnest (
      ARRAY [ 25.0,
      50.0,
      75.0,
      100.0,
      150.0,
      200.0,
      250.0,
      300.0,
      400.0,
      500.0 ]
    ) as t(depth_class)
    cross join (
      select *
      from features
    )
  order by _decision
),
tags as (
  select distinct tag_key
  from tag_position_features_mk1
),
individual_ids as (
  select tag_key,
    row_number() over () as _individual
  from tags
)
select 
    f.tag_key,
    f.epoch,
    f.lon,
    f.lat,
  _individual,
  _decision,
  depth_class = selected_depth_class as _selected,
  row_number() over () as _choice,
  _individual % 3 != 0 as _train,
  (depth_class - (avg(depth_class) over ())) / (stddev(depth_class) over ()) as depth_class,
  avg(depth_class) over () as avg_depth_class,
  stddev(depth_class) over () as std_depth_class,
  (period_progress - (avg(period_progress) over ())) / (stddev(period_progress) over ()) as period_progress,
  avg(period_progress) over () as avg_period_progress,
  stddev(period_progress) over () as std_period_progress,
  case
    when daytime then 1.0 else 0.0
  end as daytime,
  (elevation - (avg(elevation) over ())) / (stddev(elevation) over ()) as elevation,
  avg(elevation) over () as avg_elevation,
  stddev(elevation) over () as std_elevation,
  (month - (avg(month) over ())) / (stddev(month) over ()) as month,
  avg(month) over () as avg_month,
  stddev(month) over () as std_month
from expanded_features f
  inner join individual_ids i on f.tag_key = i.tag_key
order by _decision asc
```

```python
import os
import haven.db as db
import pandas as pd
import numpy as np

os.environ['HAVEN_DATABASE'] = 'haven'
os.environ['AWS_REGION'] = 'us-east-1'
os.environ['AWS_PROFILE'] = 'admin'

sql = """
select 
    avg_depth_class,
    std_depth_class,
    avg_period_progress,
    std_period_progress,
    avg_elevation,
    std_elevation,
    avg_month,
    std_month
from 
    depth_model_features_mk3
limit 1
"""
norm_params = db.read_data(sql).to_dict(orient='records')[0]
norm_params

hypercube = (
    pd.DataFrame({'period_progress': np.arange(0, 1.05, 0.05)})
    .merge(
        pd.DataFrame({'month': np.arange(1, 13)}),
        how='cross'
    )
    .merge(
        pd.DataFrame({'elevation': np.power(10, np.arange(2, 4, 0.1))}),
        how='cross'
    )
    .merge(
        pd.DataFrame({'daytime': [0.0,1.0]}),
        how='cross'
    )
)

hypercube['_individual'] = 0
hypercube['_train'] = False
hypercube['_selected'] = False
hypercube = hypercube.reset_index().rename({'index': '_decision'}, axis=1)
hypercube = hypercube.merge(
    pd.DataFrame({'depth_class': [25.0, 50.0, 75.0, 100.0, 150.0, 200.0, 250.0, 300.0, 400.0, 500.0]}),
    how='cross'
)
hypercube = hypercube.reset_index().rename({'index': '_choice'}, axis=1)

for col in ['period_progress', 'month', 'elevation', 'depth_class']:
    hypercube[f'{col}'] = (hypercube[col] - norm_params[f'avg_{col}']) / norm_params[f'std_{col}']
    hypercube[f'avg_{col}'] = norm_params[f'avg_{col}']
    hypercube[f'std_{col}'] = norm_params[f'std_{col}']
print(hypercube.shape)
hypercube.head()

db.write_data(
    hypercube,
    'depth_model_hypercube_mk3',
    ['_individual'],
)
```

```python
sql = """
with base as (
    select 
        _decision,
        _choice,
        depth_class * std_depth_class + avg_depth_class as depth_class,
        period_progress * std_period_progress + avg_period_progress as period_progress,
        elevation * std_elevation + avg_elevation as elevation,
        month * std_month + avg_month as month,
        daytime
    from 
        depth_model_hypercube_mk3
), probs as (
    select 
        _decision,
        _choice,
        probability
    from
        depth_model_mk3_hypercube_inference
)
select 
    base.*,
    probs.probability as probability
from
    base
    inner join probs 
        on base._decision = probs._decision 
        and base._choice = probs._choice

"""
hypercube = db.read_data(sql)
print(hypercube.shape)
hypercube.head()
```


# December 13, 2024

- [x] Add aliasing to spark jobs for debugging purposes (see if there are any performance issues)
- [x] Make full scale tractable
- [x] Run full month over the entire GOA
- [x] Deal with recursion depth issue by check pointing
- [x] Be able to run more than a month
- [ ] Large scale spatial animation 
- [ ] Use collected data to pose new hypotheses 

## Equal in Loss, Not in EP

| Model | Validation Loss | Loss | EP Validation | EP Training |
| --- | --- | --- | --- | --- |
| 3.2.1 | 0.168 | 0.151 | 44.8% | 46.2% | 
| 3.2.3 | 0.168 | 0.157 | 41.4% | 44.8% | 

The issue here is that the validation loss is identical and yet the EP over the validation data is not. Why is this happening? 

- [yes] Are the loss values correct?

```sql
select 
    avg(-ln(probability))
from 
    movement_model_inference_m3_a2_v3_con
where 
    _selected 
    and not _train
```

- [not really] Are specific individuals worse than others? 

They all seem quite close.

```sql
with v3_con as (
    select
        _individual,
        avg(-ln(probability)) as loss
    from 
        movement_model_inference_m3_a2_v3_con
    where 
        _selected 
        and not _train
    group by 
        1
), v1_con as (
    select
        _individual,
        avg(-ln(probability)) as loss
    from 
        movement_model_inference_m3_a2_v1_con
    where 
        _selected 
        and not _train
    group by 
        1
)
select 
    v1_con._individual,
    v1_con.loss as v1_loss,
    v3_con.loss as v3_loss,
    v3_con.loss - v1_con.loss as diff
from 
    v1_con
    left join v3_con
        on v1_con._individual = v3_con._individual
order by 
    4 desc
```

- [yes] Is the picture different if we look at average probability over the contrasts instead? 

Yes but not quite enough... 

| Model | EP over Contrasts |
| --- | --- |
| 3.2.1 | 90.5% |
| 3.2.3 | 89.7% |

- [yes] Is the picture different if we look at loss over non-contrasts? 

| Model | EP | EP Balanced | 
| --- | --- | --- |
| 3.2.1 | 1.387 | 1.346 |
| 3.2.3 | 1.396 | 1.348 | 

It's half a percent different... 

```sql
with by_ind as (
    select 
        _individual,
        avg(-ln(probability)) as loss
    from 
        movement_model_inference_m3_a2_v1
    where 
        _selected 
        and not _train
    group by 
        1
)
select 
    avg(loss)
from 
    by_ind
```

**Conclusion**: They indeed are the same from a likelihood perspective. From an EP they are not because the two are intertwined but not at all the same. 

**Outcome**: I need to report on a likelihood metric.

## Changing Contrasts

| Model | Validation Loss | Full Choices Loss |
| --- | --- | --- |
| 2.2.3 | 0.154 | 1.370 |
| 3.2.6 | 0.166 | 1.337 |

Clearly these aren't exactly lining up... and the model (in terms of features) is identical in each case. 

- [x] What happens if we run mirrored inference? 

| Model | Validation Loss | Mirror |
| --- | --- | --- |
| 2.2.3 | 0.154 | 0.167 |
| 3.2.6 | 0.166 | 0.152 |

Note too that 2.2.3 is just worse. 

**Conclusion**: different contrasts are just different samples. The models are learning the same things. 

**Outcomes**: Might make sense to be a bit heavier handed on the sampling when building contrasts. 

## Contrasts as Guides

With the above in mind, how well are the losses over the contrasts actually guiding us? 

- [x] Let's dramatically increase the number of decisions sampled per individual

- [x] Build a few different models with clearly useful
features to see if the contrast loss matches the loss we get over the real dataset

| Model | Val Loss | Train Loss | Full Val | Full Train | Val GMP | Train GMP | Test GMP |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 3.3.1 | 0.168 | 0.185 | -1.385 | -1.351 | 25.0% | 25.9% | 32.6% |
| 3.3.2 | 0.168 | 0.183 | 
| 3.3.3 | 0.160 | 0.156 | -1.384 | -1.247 | 25.1% | 28.7% | 34.1% |
| 3.3.4 | 0.156 | 0.160 | -1.340 | -1.256 | 26.2% | 28.5% | 33.6% |

**Conclusion**: yes, the magnitude of changes may not be terribly aligned but the directionality is. Also it seems the test set is quite different than the validation set which makes some general sense... 

**Outcomes** I may want to ditch the true test set... we may just not have enough coverage. 

## Is There Something Preventing the Use of Fork Length

It looks overall like this is just plain overfitting. As the number of epochs or the size of the net increases we see a decrease in loss and then at a certain point an increase in validation loss. Classic over fitting. Fork length is also suspect because, in theory, it can identify a fish quite precisely, therefore a net given enough time can learn to ID each fish in its dataset. 

So perhaps it's worth trying features that don't necessarily provide ID and see if these have similar problems. So let's try the following:

1. (3.3.5) 3.3.4 and time
2. (3.3.6) 3.3.4 and region
3. (3.3.7) 3.3.4 and time + region
4. (3.3.8) 3.3.2 and time
5. (3.3.9) 3.3.2 and region
6. (3.3.10) 3.3.2 and time + region

| Model | Val Loss | Train Loss | Features |
| --- | --- | --- | --- |
| 3.3.4 | 0.156 | 0.160 | base + food
| 3.3.5 | 0.157 | 0.151 | base + food + time
| 3.3.6 | 0.160 | 0.167 | base + food + region
| 3.3.7 | 0.166 | 0.149 | base + food + time + region
| 3.3.2 | 0.168 | 0.183 | base
| 3.3.8 | 0.156 | 0.153 | base + time
| 3.3.9 | 0.158 | 0.163 | base + region
| 3.3.10 | 0.159 | 0.162 | base + time + region

The 3.3.4, 3.3.5, 3.3.7 progression seems to indicate that time could
be useful when put alongside food and perhaps region adds something as well when you're looking at the training data. But the validation data goes in the opposite direction indicating we are very likely to be overfitting here. 

The question is whether we can eek out some addition information from, say, time or whether it and food are actually representing the same _generalized_ information. 

Looking at the hyperparameter tuning runs it does just seem like as the number of features increases the amount of validation loss increases for the same level of training loss - requiring that we push the training loss higher and higher to get equivalence between training and validation... so it looks like simple clear overfitting. 

I'll put out some really really long runs on 3.3.5 to see if somehow we can grok this. 

The only other real option I have at this point is to do some feature space reduction. 

The other interesting question is whether, on visual inspection, 3.3.4 and 3.3.8 are learning different kinds of behavior because they are indistinguishable from a performance perspective. 

```sql
with v3_con as (
    select
        _individual,
        avg(-ln(probability)) as loss
    from 
        movement_model_inference_m3_a3_v8
    where 
        _selected 
        and not _train
    group by 
        1
), v1_con as (
    select
        _individual,
        avg(-ln(probability)) as loss
    from 
        movement_model_inference_m3_a3_v4
    where 
        _selected 
        and not _train
    group by 
        1
)
select 
    v1_con._individual,
    v1_con.loss as v1_loss,
    v3_con.loss as v3_loss,
    v3_con.loss - v1_con.loss as diff
from 
    v1_con
    left join v3_con
        on v1_con._individual = v3_con._individual
order by 
    4 desc
```

Watching the animations it has occurred to me that time almost perfectly identifies place... This is likely why the model is able to use time so well... but it also feels a bit like cheating. 

## Will DropOut Help?

We're struggling with overfitting, let's try adding in dropout. 

I just got 0.154 on a model with the same features as 3.3.5 on a network with three layers of 16 neurons and we never 
actually started seeing overfitting during training... Amazing!



## Time to Digest

- Are these features really cheat cards?
- What are they telling me that I need to pull from other kinds of information?
- Did this grok? 
- What if I binned forked length? Is it still indicative of movement levels?
- Are the regions and the spots of release also correlated? 

All in all though this is definitely working :) I have a solid process for exploring information additions. 



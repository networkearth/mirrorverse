# December 20, 2024

## Headlines

- Dropout is very effective at eliminating overfitting in these models
- Our models are doing a good job of learning how fish move and not just the rate at which they "stay put"
- Overall the model is 5x better than random on GMP! And it's 3x better on decisions that involve moving. 

## Notes from the Week

Using [this article](https://towardsdatascience.com/designing-your-neural-networks-a5e4617027ed) as inspiration. 

### Drop Out

#### Basic Test

The main problem I was having at the end of last week was overfitting. As I was adding more and more features I was having to make the net smaller and smaller in order to avoid it. Drop out is specifically designed to help avoid this. 

I went ahead and trained a series of models all three layers deep of varying layer size (8, 16, 24, 32) and with varying levels of dropout (0%, 10%, 20%, 30%). The features incorporated were `normed_distance`, `movement_heading`, `normed_log_mlt`, `normed_log_npp`, `cos_time`, `sin_time` - a combination that had been giving me overfitting troubles before. (Version 3.3.12)

![without dropout](2024_12_20/without_dropout.png)

![with dropout](2024_12_20/with_dropout.png)

![training loss with dropout](2024_12_20/tloss_with_dropout.png)

Dropout takes care of the problem for us!

With the dropout at 30% it even works for 32-32-32 and gives us the lowest validation loss we've seen yet (at least briefly)!

![32-32-32](2024_12_20/32_with_dropout.png)

This is fantastic as it'll allow us to focus on the other hyperparameters rather than trying to find the perfect network size. 

#### All the Features? 

Given this is helping us this much, can we go ahead and just throw all the features into the same pot? 

We'll run through:

| Hyperparameter | Options |
| --- | --- |
| Epochs | 50 |
| Batch Size | 500 |
| Neurons per Layer | 24, 32, 48 |
| Layers | 3 |
| Dropout % | 20, 30, 40 |

| Model | Val Loss | Loss | Features |
| --- | --- | --- | --- |
| 3.3.13 | 0.158 | 0.166 | "normed_distance", "movement_heading", "normalized_fork_length", "normed_home_lat", "normed_home_lon", "cos_time", "sin_time", "normed_log_mlt", "normed_log_npp" |
| 3.3.14 | 0.152 | 0.175 | "normed_distance", "movement_heading", "normed_log_mlt", "normed_log_npp" |

From this alone it looks like there's some overfitting still going on with all of these features. 

The best from 3.3.14

![14 best](2024_12_20/14_best.png)

Looks like it was still learning! (24-24-24, dropout 30%)

Looking at the best from 3.3.13

![13 best](2024_12_20/13_best.png)

it doesn't necessarily seem like standard overfitting is happening here... more that it's having trouble learning the same generalizable pattern as above. Also interesting this "best" had the same architecture as the best from 3.3.14 which makes me wonder if the 32 and 48 archs were themselves overfitting... 

![13 48 best](2024_12_20/13_48_best.png)

That is the best version of the 48-48-48's... and yea that's definitely overfitting. Let's get more aggressive with the dropout. 

| Hyperparameter | Options |
| --- | --- |
| Epochs | 50 |
| Batch Size | 500 |
| Neurons per Layer | 32, 48 |
| Layers | 3 |
| Dropout % | 30, 40, 50, 60 |

My best validation loss here was a 0.16 and the graphs show that the overfitting issue has been addressed. However there doesn't seem to be an advantage in terms of performance as compared to the 24-24-24 models from 3.3.14. 

**Conclusions** Dropout works as a tool to push the model away from overfitting. However it is still not allowing us to combine all features usefully. 

### Random Starts

Something that's been starting to bother me is a question of how much variability is there in these fits just based on start "position" alone. I.e. for the same hyperparameters how much variability is there in the outcomes? 

| Hyperparameter | Options |
| --- | --- |
| Epochs | 100 |
| Batch Size | 500 |
| Neurons per Layer | 16, 24, 32 |
| Layers | 3 |
| Dropout % | 20, 30, 40 |
| Random Starts | ~20 | 

**Full Features 3.3.16**
| Neurons | Dropout % | 90th Val Loss - Min Val Loss | Min Val Loss |
| --- | --- | --- | --- |
| 16 | 20 | 0.013 | 0.159 |
| | 30 | 0.021 | 0.153 | 
| | 40 | 0.021 | 0.159 | 
| 24 | 20 | 0.02 | 0.159 | 
| | 30 | 0.014 | 0.163 |
| | 40 | 0.021 | 0.156 | 
| 32 | 20 | 0.019 | 0.170 |
| | 30 | 0.025 | 0.163 | 
| | 40 | 0.012 | 0.158 | 

**Limited Features 3.3.17**
| Neurons | Dropout % | 90th Val Loss - Min Val Loss | Min Val Loss |
| --- | --- | --- | --- |
| 16 | 20 | 0.012 | 0.152 |
| | 30 | 0.008 | 0.156 | 
| | 40 | 0.022 | 0.153 | 
| 24 | 20 | 0.006 | 0.152 | 
| | 30 | 0.012 | 0.151 |
| | 40 | 0.016 | 0.152 | 
| 32 | 20 | 0.008 | 0.153 |
| | 30 | 0.009 | 0.152 | 
| | 40 | 0.010 | 0.152 | 

It does seem randomness a significant part to play. Interestingly if I dropped the percentile to the 80th the distance to the min was consistent at or less than 0.01. All of this to say that I definitely need to take a large array of samples in order to find a global minima. 

Also somewhat oddly to me there is no real pattern here. Sometimes additional depth is helpful, sometimes it's not. However if the starting point is so "sensitive" 

### Depth

Beyond increasing the number of neurons per layer, we can also adjust the number of layers in our model. Let's go ahead and experiment with this as well. 

| Hyperparameter | Options |
| --- | --- |
| Epochs | 50 |
| Batch Size | 500 |
| Neurons per Layer | 16, 24 |
| Layers | 2, 3, 4 |
| Dropout % | 20, 30 |
| Random Starts | ~5 | 

Unfortunately I messed up a config and what was supposed to be 3.3.18 got saved in 3.3.16... thankfully we can use the presence of `num_layers` in the config to sort out the two. 

**Full Features 3.3.16 (supposed to be 3.3.18)**
![Full Features Depth Experiment](2024_12_20/full_features_depth.png)

**Limited Features 3.3.19**
![Limited Features Depth Experiment](2024_12_20/limited_features_depth.png)

Looking across these it seems that the range of values here represents a change of ~0.01-0.014 to the val_loss. 

This means that random starts and actual hyperparameter tuning (depth, dropout rate, and neurons per layer) are having somewhat equivalent effects. 

### Bridging Thoughts

We're no longer overfitting, underfitting perhaps but it is hard to say given how little independent data we really have... A few observations stand out to me:

1. There is very little we're actually really doing as we change these hyperparameters and these features. We're looking at moving from 0.17 -> 0.15 which on the outset feels like a lot but when you exponentiate that it's really 84.4% -> 86.1%. 
2. If sampled incorrectly, or if using the "wrong" hyperparameters we can create changes as large as that our features give us... 
3. There is quite a big difference it seems between the distribution in the validation, training, and test datasets. 

All to say I feel like I'm hitting the 20% of the pareto rule... problem is I'm not entirely sure why. 

I'm fairly confident it isn't the model at this point given the fact that changing all of these major aspects is not giving me a whole load of gains. However what is rather troubling to me is the fact that, in the contrasts, the model is effectively 85% accurate... 

It makes me think that I've still got some kind of class imbalance problem that's preventing the model from learning any of the more interesting things. Or that somehow using contrasts is a bad idea... (however I've got evidence that isn't really the case...)

So here are my possible culprits:
1. Patterns do not in fact generalize across these datasets at this level of cardinality 
2. The math of contrasts does not in fact work out
3. We have some kind of decision imbalance preventing the model from learning

And the first is probably the most interesting to me because while I've created 100's of thousands of data points (and could create millions with augmentation) I've only got 20 fish in that validation set. 

Another random thought that's been floating through my head is whether these fish are in fact acting incredibly randomly and my mind is giving some pattern to their behavior... 

For example one of the big patterns is the sudden migration south by the group by the glaciers. While it certainly looks to me very deliberate, what if it's actually just randomness with a very small bias. If I allow a creature to random walk but bias specific aspects of that walk just slightly, they will indeed make very steady and consistent progress in that direction - they simply have to. 

In this same line of thinking, if there are decisions that are extremely unlikely, they will not actually end up in the dataset all that often. The algorithm can still learn to completely avoid them, but they won't show up much in the score... the score will be dominated by choices that are common - like staying in the cell you're already in... 

Hold on... what do my scores look like when distance is not the reason for staying put... 

I just pulled 3.3.6 which is the best feature set we have right now reasonably well trained and the stats for decisions that aren't based on staying where you were kind of tell it all - 5.2% geometric mean of the probability - it's not better than shakespeare's monkey. Yet it is also telling that it's finding a ~65% chance of just staying put. 

All of this gives me a couple of directions forward. 

1. I can test the classification bias issue (and the lack of generalization) by training only on decisions where the selected decision did not mean "stay in place".
2. If that totally fails I can then see if this kind of modeling can even pick up on small biases by creating some fake data (where I know how it should behave) and testing on that. 
3. I can include "perimeters" in that fake dataset to see whether low/zero probability choices actually get noted as such by the model. 

### When Distance Cannot Cheat

Welcome to m4. 

| Model | Val Loss (Contrast) | Loss (Contrast) | Val GMP | GMP |
| --- | --- | --- | --- | --- |
| 4.1.1 | 0.198 | 0.249 | 17.4% | 17% | 

That's ~3x better than we'd expect from our monkey! 

But this got me thinking... When I evaluated the performance of the models trained over _all_ of the data I evaulated their probabilities... what I really needed to do was remove the odds for the `distance=0` choices, reevaluate the probabilities, and _then_ look at the GMP for `distance>0` selections. Let's see what that looks like. 

| Model | Val GMP | GMP |
| --- | --- | --- |
| 3.2.6 |  16.7% | 15.6% | 

Haha! We are indeed biasing toward the right decisions it's just that the power of staying still is so strong it seems to wash out the probabilities for moving away from the current position. 

`(1-0.65)0.17=0.06`

The model is indeed learning about more than just when to stay and when to go. And it seems to be doing so quite well. The problem is I simply can't see the effects. 

Just out of curiousity let's see what happens when you use the full rather than limited feature set just on these subset of contrasts. 

| Model | Val Loss (Contrast) | Loss (Contrast) | 
| --- | --- | --- | 
| 4.1.2 | 0.21 | 0.213 | 

It's definitely learning something new based on the training loss, but doesn't seem those patterns are transferrable to the validation set.

Overall though I think these results remove my concerns that the contrasts are not working. The model is learning what it can reasonably well. It just seems that at the end of the data we don't have a ton of fish and so we have significant differences between our validation and training sets. 

### Bridge

The answer to my question of how to see these "hidden" aspects of the models is, I think, simple. We already noted how we can look for new problems to solve by splitting up our data into "behaviors" - individuals at specific times doing specific things - and then look at the validation loss on those specifically. This remains true - we could take specific groups of salmon, or times when they exhibit specific behaviors, and select models that are particularly good at those bits. 

But likewise what this whole distance bit has shown is that you can select specific groupings of _choices_ and not just specific groupings of decisions. Then because we're working with odds we can reevaluate the probabilities with the choices we don't care about removed and eval on that instead. 

Furthermore as we've just seen we can train models specific to any one of these cases and then, when we've found features that work, try to combine them. If the combination doesn't work first round we can add artificial features that tells our model which group is which to make sure our architecture is capable of telling the difference before just concluding there's a missing piece of information. 

All of which is to say we can continue this model development by looking at loads of different GMP stats across various different choice and behavior groupings. And we can break up the study by studying each of these groups individually and then trying to stitch them back together. 

Sweet!








{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/01 14:14:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"local\")\n",
    "    .appName(\"SQLite JDBC\")\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"/workspaces/mirrorverse/sqlite-jdbc-3.34.0.jar\")\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"/workspaces/mirrorverse/sqlite-jdbc-3.34.0.jar\")\n",
    "    .getOrCreate())\n",
    "\n",
    "ps_conn = \"jdbc:sqlite:/workspaces/mirrorverse/mirrorverse.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mirrorverse.models.chinook_depth.spark import build_training_data\n",
    "\n",
    "depth_classes = [25, 50, 75, 100, 150, 200, 250, 300, 400, 500]\n",
    "features = [\"depth_class\", \"month\", \"daytime\", \"period_progress\", \"elevation\"]\n",
    "train_dir = \"train\"\n",
    "test_dir = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "build_training_data(\n",
    "    spark,\n",
    "    ps_conn,\n",
    "    depth_classes=np.array(depth_classes),\n",
    "    features=features,\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    split=0.8,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'batch_size': 10000, 'epochs': 5, 'layers': <function __main__.<lambda>()>},\n",
       " {'batch_size': 5000, 'epochs': 5, 'layers': <function __main__.<lambda>()>}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mirrorverse.models.chinook_depth.keras import load_data, build_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def build_randomized_param_sets(param_grids, M, max_attempts):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - param_grids: dict, parameter names to grid of values\n",
    "    - M: int, number of parameter sets to generate\n",
    "    - max_attempts: int, maximum number of attempts to generate a unique parameter set\n",
    "\n",
    "    Outputs:\n",
    "    - list of dicts, parameter sets\n",
    "    \"\"\"\n",
    "    param_sets = []\n",
    "    attempts = 0\n",
    "    while len(param_sets) < M:\n",
    "        assert attempts < max_attempts\n",
    "\n",
    "        param_set = {}\n",
    "        for param, grid in param_grids.items():\n",
    "            param_set[param] = np.random.choice(grid)\n",
    "        if param_set in param_sets:\n",
    "            attempts += 1\n",
    "        else:\n",
    "            attempts = 0\n",
    "            param_sets.append(param_set)\n",
    "\n",
    "    return param_sets\n",
    "\n",
    "grids = {\n",
    "    \"batch_size\": [100, 500, 1000, 5000, 10000],\n",
    "    \"epochs\": [10],\n",
    "    \"layers\": [\n",
    "        lambda: [Dense(64, activation='relu'), Dense(32, activation='relu'), Dense(16, activation='relu')],\n",
    "        lambda: [Dense(32, activation='relu'), Dense(16, activation='relu'), Dense(8, activation='relu')],\n",
    "        lambda: [Dense(16, activation='relu'), Dense(8, activation='relu'), Dense(4, activation='relu')],\n",
    "\n",
    "        lambda: [Dense(16, activation='relu'), Dense(8, activation='relu'), Dense(16, activation='relu')],\n",
    "        lambda: [Dense(8, activation='relu'), Dense(16, activation='relu'), Dense(8, activation='relu')],\n",
    "        lambda: [Dense(16, activation='relu'), Dense(16, activation='relu'), Dense(16, activation='relu')],\n",
    "    ]\n",
    "}\n",
    "\n",
    "param_sets = build_randomized_param_sets(grids, 5 * 6, 10)\n",
    "param_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "\n",
    "mlflow.set_experiment(\"/chinook_depth_model_tuning\")\n",
    "\n",
    "# Turn off autologging.\n",
    "mlflow.tensorflow.autolog(disable=True)\n",
    "\n",
    "N = len(depth_classes)\n",
    "for param_set in param_sets:\n",
    "    batch_size = param_set[\"batch_size\"]\n",
    "    epochs = param_set[\"epochs\"]\n",
    "    layers = param_set[\"layers\"]()\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "\n",
    "        train = load_data(train_dir, N, features, batch_size=batch_size, shuffle_buffer_size=10000)\n",
    "        test = load_data(test_dir, N, features, batch_size=batch_size, shuffle_buffer_size=10000)\n",
    "\n",
    "        for i, layer in enumerate(layers):\n",
    "            mlflow.log_param(f\"layer_{i}_units\", layer.get_config()['units'])\n",
    "\n",
    "        model, layers = build_model(N, features, layers)\n",
    "\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "    \n",
    "        history = model.fit(train, validation_data=test, epochs=epochs)\n",
    "\n",
    "        for metric_name, metrics in history.history.items():\n",
    "            for i, metric in enumerate(metrics):\n",
    "                mlflow.log_metric(metric_name, metric, step=i)\n",
    "            mlflow.log_metric(f\"final_{metric_name}\", metrics[-1])\n",
    "            mlflow.log_metric(f\"min_{metric_name}\", min(metrics))\n",
    "\n",
    "        mlflow.keras.log_model(model, \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run Full Inference\n",
    "\n",
    "from mirrorverse.models.chinook_depth.spark import run_inference\n",
    "\n",
    "query = \"\"\"\n",
    "with h3 as (\n",
    "    select distinct\n",
    "        h3_level_4_key \n",
    "    from \n",
    "        tag_tracks\n",
    "), time as (\n",
    "    select distinct\n",
    "        date_key as epoch\n",
    "    from \n",
    "        dates\n",
    ")\n",
    "select \n",
    "    t.epoch,\n",
    "    h3.h3_level_4_key\n",
    "from \n",
    "    h3\n",
    "    cross join time t\n",
    "\"\"\"\n",
    "\n",
    "run_inference(\n",
    "    spark, \n",
    "    ps_conn,\n",
    "    depth_classes,\n",
    "    features,\n",
    "    query,\n",
    "    \"train/normalization_parameters.json\",\n",
    "    \"/workspaces/mirrorverse/mlartifacts/539696127796712602/4b66bdcb3d194395be7bf40225d1db2f/artifacts/models/data/model.keras\",\n",
    "    \"depth_predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h3_level_4_key</th>\n",
       "      <th>epoch</th>\n",
       "      <th>depth_class</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>594692656546709503</td>\n",
       "      <td>1644796800</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.359553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>594692656546709503</td>\n",
       "      <td>1644883200</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.359553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>594692656546709503</td>\n",
       "      <td>1644969600</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.359553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>594692656546709503</td>\n",
       "      <td>1645056000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.359553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>594692656546709503</td>\n",
       "      <td>1645142400</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.359553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457535</th>\n",
       "      <td>594988459534319615</td>\n",
       "      <td>1674777600</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.001831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457536</th>\n",
       "      <td>594988459534319615</td>\n",
       "      <td>1675123200</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.001831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457537</th>\n",
       "      <td>594988459534319615</td>\n",
       "      <td>1675641600</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.001769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457538</th>\n",
       "      <td>594988459534319615</td>\n",
       "      <td>1675728000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.001769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457539</th>\n",
       "      <td>594988459534319615</td>\n",
       "      <td>1676246400</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.001769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2457540 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             h3_level_4_key       epoch  depth_class  probability\n",
       "0        594692656546709503  1644796800         25.0     0.359553\n",
       "1        594692656546709503  1644883200         25.0     0.359553\n",
       "2        594692656546709503  1644969600         25.0     0.359553\n",
       "3        594692656546709503  1645056000         25.0     0.359553\n",
       "4        594692656546709503  1645142400         25.0     0.359553\n",
       "...                     ...         ...          ...          ...\n",
       "2457535  594988459534319615  1674777600        500.0     0.001831\n",
       "2457536  594988459534319615  1675123200        500.0     0.001831\n",
       "2457537  594988459534319615  1675641600        500.0     0.001769\n",
       "2457538  594988459534319615  1675728000        500.0     0.001769\n",
       "2457539  594988459534319615  1676246400        500.0     0.001769\n",
       "\n",
       "[2457540 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_conn = \"sqlite:////workspaces/mirrorverse/mirrorverse.db\"\n",
    "df = pd.read_sql(\"select * from depth_predictions where epoch > (1676246400 -(3600 * 24 * 365)) \", pd_conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "```bash\n",
    "apt-get update\n",
    "apt install default-jre\n",
    "\n",
    "curl -O https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.34.0/sqlite-jdbc-3.34.0.jar\n",
    "\n",
    "\n",
    "Needed conda to get h5py installed correctly\n",
    "\n",
    "mkdir -p ~/miniconda3\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh -O ~/miniconda3/miniconda.sh\n",
    "bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\n",
    "rm -rf ~/miniconda3/miniconda.sh\n",
    "\n",
    "~/miniconda3/bin/conda init bash\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
